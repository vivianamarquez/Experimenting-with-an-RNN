{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('AllTrumpSpeechesCleaned.txt', encoding=\"utf8\") as file:\n",
    "    speeches = file.read()\n",
    "text = speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((256, 100), (256, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 256\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "embedding_dim = 64\n",
    "\n",
    "rnn_units = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (256, None, 64)           5760      \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (256, None, 512)          886272    \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (256, None, 512)          1574400   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (256, None, 90)           46170     \n",
      "=================================================================\n",
      "Total params: 2,512,602\n",
      "Trainable params: 2,512,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "    vocab_size = len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "      return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = 'training_checkpoints/'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_prefix,\n",
    "        save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 384s 6s/step - loss: 0.6976\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=1\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "p_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "\n",
    "p_model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = 1000\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the word returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted word as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return(start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tremendous cash you like Congress Caterpillar mater. Her pupal patens. And now he’s the worst politicians should get out.\n",
      "But then when are you undernic our women and wo sold and you’ll never for the vote. The policies here are knerc, lies that we’re up to hardloke and then the best in violence. Their people really taken care of plant through will be going to stop it and he’s not going to happen.\n",
      "And you know what o know this? No?\n",
      "You got tos rich is about with two carery. Recount is avaitybody. Because when he said, “Oh my best. Okay? You know, I’m me, today see that I can make it vire less stapped. That won’t come in.\n",
      "But we’re going to Mexico great and that they can do to Michigan. Bet it did with two days.\n",
      "People come out fast and somebody will take the appreciate in. The group hearts the heroic of the grateorm. Because if that was without cutting? They pays benefit leagest people in the world alones and their liest to run for – no good to show that very walls desperately from a friend of min\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(p_model, start_string=u\"Tremendous \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will be consraluse totally illegal immigration – that you’ve totally called and make it up. Instead of study. The country go, and we could rnCw is rigged.\n",
      "Here is drnigration. Make is great Trump unemestave because we have rising our country and bleat me. Gr, and they were the oil, keep by people with Chicago, many of the FBI to Washington Bridge living trood in Michigan – safe totacking me on trade and becoming the game in a woman instive lies and then we win the same direction neit spend away on The Apprentice, amazing people.\n",
      "We are just turned with the voters, for loyto, Cruze sendeng to strible. They want to go back to.\n",
      "And she said “Oh, TPP. ThanHe made the American people, and we are going to start winning from the real efform.\n",
      "And he wasn’t understand.\n",
      "But it ter censured that it’s alone going to say MacDoll Japan.\n",
      "Legally dat lies. Doctore even though a rene? Remember? He said “It’s no ble the one chance of Dirminals.” And then the people like Christmashe profit. Believe me. Americ\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(p_model, start_string=u\"You will \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will sig decade in the world. Left Johnson, hotels would say nI think I’m farlent for the United States — with Japan.\n",
      "So I’de ated the money becoming conservated cities throw reconsting up lik the peopleh war right now is apared by far. And he was going to class and run because he was released and through weeks, and they discussed great veterans I’ve been said 10,000 7000 and the dancer radical Islamic terrorists the hell out so badly for by far to growe this citizens of the United States to pay for it’s all immigration natural gas determine and general long and here’s unfair safe neighbodyou’ll a speech in terms of noching right after plant…\n",
      "And we’re going to start winning too much loy”, I appreciate it.\n",
      "And you’re going to be losing phery – which is going to be right over there? They say.\n",
      "But I am running to build a great, great company but I think that was very nice.\n",
      "Big, big difference between China, they’re in the san the way they get in. Nobody really done a big chunk of what it was \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(p_model, start_string=u\"I will \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
